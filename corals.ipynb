{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca6eae11-866f-4be1-98f3-4d6e26b5d9aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¾ Dataset Info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1252 entries, 0 to 1251\n",
      "Data columns (total 21 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Site_ID              1252 non-null   int64  \n",
      " 1   Latitude_Degrees     1252 non-null   float64\n",
      " 2   Longitude_Degrees    1252 non-null   float64\n",
      " 3   Ocean_Name           1252 non-null   object \n",
      " 4   Ecoregion_Name       1252 non-null   object \n",
      " 5   Exposure             1252 non-null   object \n",
      " 6   Turbidity            1252 non-null   float64\n",
      " 7   Cyclone_Frequency    1252 non-null   float64\n",
      " 8   Date_Month           1252 non-null   int64  \n",
      " 9   Date_Year            1252 non-null   int64  \n",
      " 10  Percent_Bleaching    1252 non-null   float64\n",
      " 11  Temperature_Mean     1252 non-null   object \n",
      " 12  Windspeed            1252 non-null   object \n",
      " 13  SSTA                 1252 non-null   object \n",
      " 14  SSTA_DHW             1252 non-null   object \n",
      " 15  TSA                  1252 non-null   object \n",
      " 16  TSA_DHWMean          1252 non-null   object \n",
      " 17  Date                 1252 non-null   object \n",
      " 18  Site_Comments        1252 non-null   object \n",
      " 19  Sample_Comments      1252 non-null   object \n",
      " 20  Bleaching_condition  1252 non-null   object \n",
      "dtypes: float64(5), int64(3), object(13)\n",
      "memory usage: 205.5+ KB\n",
      "None\n",
      "\n",
      "ðŸ“Š Summary Statistics:\n",
      "\n",
      "                      count unique  \\\n",
      "Site_ID              1252.0    NaN   \n",
      "Latitude_Degrees     1252.0    NaN   \n",
      "Longitude_Degrees    1252.0    NaN   \n",
      "Ocean_Name             1252      5   \n",
      "Ecoregion_Name         1252     86   \n",
      "Exposure               1252      3   \n",
      "Turbidity            1252.0    NaN   \n",
      "Cyclone_Frequency    1252.0    NaN   \n",
      "Date_Month           1252.0    NaN   \n",
      "Date_Year            1252.0    NaN   \n",
      "Percent_Bleaching    1252.0    NaN   \n",
      "Temperature_Mean       1252    388   \n",
      "Windspeed              1252     14   \n",
      "SSTA                   1252    344   \n",
      "SSTA_DHW               1252    566   \n",
      "TSA                    1252    440   \n",
      "TSA_DHWMean            1252    142   \n",
      "Date                   1252    146   \n",
      "Site_Comments          1252    573   \n",
      "Sample_Comments        1252    573   \n",
      "Bleaching_condition    1252      3   \n",
      "\n",
      "                                                                   top freq  \\\n",
      "Site_ID                                                            NaN  NaN   \n",
      "Latitude_Degrees                                                   NaN  NaN   \n",
      "Longitude_Degrees                                                  NaN  NaN   \n",
      "Ocean_Name                                                    Atlantic  546   \n",
      "Ecoregion_Name                               Belize and west Caribbean  406   \n",
      "Exposure                                                     Sheltered  572   \n",
      "Turbidity                                                          NaN  NaN   \n",
      "Cyclone_Frequency                                                  NaN  NaN   \n",
      "Date_Month                                                         NaN  NaN   \n",
      "Date_Year                                                          NaN  NaN   \n",
      "Percent_Bleaching                                                  NaN  NaN   \n",
      "Temperature_Mean                                                300.88  152   \n",
      "Windspeed                                                            4  340   \n",
      "SSTA                                                              0.51   22   \n",
      "SSTA_DHW                                                             0  286   \n",
      "TSA                                                                 -1   20   \n",
      "TSA_DHWMean                                                       0.24  101   \n",
      "Date                                                        15-04-1998  138   \n",
      "Site_Comments        Number of colonies observed: 18 ; Number of bl...   27   \n",
      "Sample_Comments      Number of colonies observed: 18 ; Number of bl...   27   \n",
      "Bleaching_condition  Bleaching percentage averaged from code: Mild ...  560   \n",
      "\n",
      "                            mean         std      min        25%      50%  \\\n",
      "Site_ID              4686.981629  2740.52233      1.0    1890.75   6204.5   \n",
      "Latitude_Degrees        5.658071    16.80852 -28.7001 -14.335325  13.4715   \n",
      "Longitude_Degrees       7.905756   109.29317  -177.35   -87.4704 -67.0271   \n",
      "Ocean_Name                   NaN         NaN      NaN        NaN      NaN   \n",
      "Ecoregion_Name               NaN         NaN      NaN        NaN      NaN   \n",
      "Exposure                     NaN         NaN      NaN        NaN      NaN   \n",
      "Turbidity               0.071081     0.06808   0.0176   0.033375  0.04745   \n",
      "Cyclone_Frequency      49.218051    8.174856    21.71    45.6925     49.1   \n",
      "Date_Month              5.981629    2.857469      1.0        4.0      5.0   \n",
      "Date_Year            2001.428914    4.336069   1980.0     1998.0   2002.0   \n",
      "Percent_Bleaching      32.859824   29.619262      5.5        5.5     30.5   \n",
      "Temperature_Mean             NaN         NaN      NaN        NaN      NaN   \n",
      "Windspeed                    NaN         NaN      NaN        NaN      NaN   \n",
      "SSTA                         NaN         NaN      NaN        NaN      NaN   \n",
      "SSTA_DHW                     NaN         NaN      NaN        NaN      NaN   \n",
      "TSA                          NaN         NaN      NaN        NaN      NaN   \n",
      "TSA_DHWMean                  NaN         NaN      NaN        NaN      NaN   \n",
      "Date                         NaN         NaN      NaN        NaN      NaN   \n",
      "Site_Comments                NaN         NaN      NaN        NaN      NaN   \n",
      "Sample_Comments              NaN         NaN      NaN        NaN      NaN   \n",
      "Bleaching_condition          NaN         NaN      NaN        NaN      NaN   \n",
      "\n",
      "                          75%     max  \n",
      "Site_ID                6448.0  9690.0  \n",
      "Latitude_Degrees      19.8805   36.75  \n",
      "Longitude_Degrees    127.7952  179.85  \n",
      "Ocean_Name                NaN     NaN  \n",
      "Ecoregion_Name            NaN     NaN  \n",
      "Exposure                  NaN     NaN  \n",
      "Turbidity              0.0797  0.9785  \n",
      "Cyclone_Frequency     51.9625   105.8  \n",
      "Date_Month                8.0    12.0  \n",
      "Date_Year              2005.0  2010.0  \n",
      "Percent_Bleaching        75.0    75.0  \n",
      "Temperature_Mean          NaN     NaN  \n",
      "Windspeed                 NaN     NaN  \n",
      "SSTA                      NaN     NaN  \n",
      "SSTA_DHW                  NaN     NaN  \n",
      "TSA                       NaN     NaN  \n",
      "TSA_DHWMean               NaN     NaN  \n",
      "Date                      NaN     NaN  \n",
      "Site_Comments             NaN     NaN  \n",
      "Sample_Comments           NaN     NaN  \n",
      "Bleaching_condition       NaN     NaN  \n",
      "\n",
      "ðŸ”¹ Sample Rows:\n",
      "\n",
      "   Site_ID  Latitude_Degrees  Longitude_Degrees Ocean_Name  \\\n",
      "0       29          -23.4439           151.9286    Pacific   \n",
      "1       63          -18.7502           147.2688    Pacific   \n",
      "2       65          -17.2661           146.3636    Pacific   \n",
      "3       81          -20.1930           150.0820    Pacific   \n",
      "4       85          -20.1559           149.0719    Pacific   \n",
      "\n",
      "                            Ecoregion_Name   Exposure  Turbidity  \\\n",
      "0              Southern Great Barrier Reef  Sheltered     0.0731   \n",
      "1  Central and northern Great Barrier Reef  Sheltered     0.0458   \n",
      "2  Central and northern Great Barrier Reef  Sheltered     0.0755   \n",
      "3  Central and northern Great Barrier Reef  Sheltered     0.0743   \n",
      "4  Central and northern Great Barrier Reef  Sheltered     0.0627   \n",
      "\n",
      "   Cyclone_Frequency  Date_Month  Date_Year  ...  Temperature_Mean Windspeed  \\\n",
      "0              51.57           2       1998  ...            297.35         5   \n",
      "1              44.20           2       1998  ...            299.18         2   \n",
      "2              46.35           2       1998  ...            299.52         3   \n",
      "3              48.00           3       2002  ...            298.52         8   \n",
      "4              46.13           3       2002  ...            298.55         7   \n",
      "\n",
      "   SSTA SSTA_DHW    TSA TSA_DHWMean        Date  \\\n",
      "0  5.48    31.79   5.24       10.05  15-02-1998   \n",
      "1  2.05     4.43      2        0.44  15-02-1998   \n",
      "2  0.85     2.49   0.73         0.4  15-02-1998   \n",
      "3  0.46     3.14  -0.02        0.24  15-03-2002   \n",
      "4  0.41     4.91  -0.09        0.37  15-03-2002   \n",
      "\n",
      "                                       Site_Comments  \\\n",
      "0  Acropora colonies (staghorn corals) are mainly...   \n",
      "1                             80% of corals bleached   \n",
      "2                              significant bleaching   \n",
      "3                             lots of white on slope   \n",
      "4  ie Minstrel rock on nth end. South Deloraine w...   \n",
      "\n",
      "                                     Sample_Comments  \\\n",
      "0  Acropora colonies (staghorn corals) are mainly...   \n",
      "1                             80% of corals bleached   \n",
      "2                              significant bleaching   \n",
      "3                             lots of white on slope   \n",
      "4  ie Minstrel rock on nth end. South Deloraine w...   \n",
      "\n",
      "                                 Bleaching_condition  \n",
      "0  Bleaching percent averaged from code: Severe (...  \n",
      "1  Bleaching percent averaged from code: Severe (...  \n",
      "2  Bleaching percent averaged from code: Severe (...  \n",
      "3  Bleaching percent averaged from code: Severe (...  \n",
      "4  Bleaching percent averaged from code: Severe (...  \n",
      "\n",
      "[5 rows x 21 columns]\n",
      "\n",
      "â— Missing Values:\n",
      "\n",
      "Site_ID                0\n",
      "Latitude_Degrees       0\n",
      "Longitude_Degrees      0\n",
      "Ocean_Name             0\n",
      "Ecoregion_Name         0\n",
      "Exposure               0\n",
      "Turbidity              0\n",
      "Cyclone_Frequency      0\n",
      "Date_Month             0\n",
      "Date_Year              0\n",
      "Percent_Bleaching      0\n",
      "Temperature_Mean       0\n",
      "Windspeed              0\n",
      "SSTA                   0\n",
      "SSTA_DHW               0\n",
      "TSA                    0\n",
      "TSA_DHWMean            0\n",
      "Date                   0\n",
      "Site_Comments          0\n",
      "Sample_Comments        0\n",
      "Bleaching_condition    0\n",
      "dtype: int64\n",
      "\n",
      "ðŸŒŠ Bleaching Level Distribution:\n",
      "\n",
      "Bleaching_Level\n",
      "Mild        560\n",
      "Severe      381\n",
      "Moderate    311\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ðŸ“ˆ Correlation Matrix (numerical columns only):\n",
      "\n",
      "                    Site_ID  Latitude_Degrees  Longitude_Degrees  Turbidity  \\\n",
      "Site_ID            1.000000          0.520967          -0.376681  -0.047724   \n",
      "Latitude_Degrees   0.520967          1.000000          -0.556479  -0.127895   \n",
      "Longitude_Degrees -0.376681         -0.556479           1.000000   0.248618   \n",
      "Turbidity         -0.047724         -0.127895           0.248618   1.000000   \n",
      "Cyclone_Frequency -0.040547          0.220778          -0.022091  -0.074627   \n",
      "Date_Month         0.212857          0.523760          -0.279080  -0.031262   \n",
      "Date_Year          0.196007          0.221160          -0.294027  -0.220385   \n",
      "Percent_Bleaching -0.106850         -0.254901           0.236921   0.152383   \n",
      "\n",
      "                   Cyclone_Frequency  Date_Month  Date_Year  Percent_Bleaching  \n",
      "Site_ID                    -0.040547    0.212857   0.196007          -0.106850  \n",
      "Latitude_Degrees            0.220778    0.523760   0.221160          -0.254901  \n",
      "Longitude_Degrees          -0.022091   -0.279080  -0.294027           0.236921  \n",
      "Turbidity                  -0.074627   -0.031262  -0.220385           0.152383  \n",
      "Cyclone_Frequency           1.000000    0.265680  -0.068080          -0.020267  \n",
      "Date_Month                  0.265680    1.000000   0.023604          -0.073482  \n",
      "Date_Year                  -0.068080    0.023604   1.000000          -0.211612  \n",
      "Percent_Bleaching          -0.020267   -0.073482  -0.211612           1.000000  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Step 2: Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# Step 3: Display basic info\n",
    "print(\"ðŸ§¾ Dataset Info:\\n\")\n",
    "print(df.info())\n",
    "\n",
    "# Step 4: Display summary statistics\n",
    "print(\"\\nðŸ“Š Summary Statistics:\\n\")\n",
    "print(df.describe(include='all').transpose())\n",
    "\n",
    "# Step 5: Display first few rows\n",
    "print(\"\\nðŸ”¹ Sample Rows:\\n\")\n",
    "print(df.head())\n",
    "\n",
    "# Step 6: Check for missing values\n",
    "print(\"\\nâ— Missing Values:\\n\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "# Step 7: If you already have the 'Percent_Bleaching' column,\n",
    "# classify severity and check distribution\n",
    "if 'Percent_Bleaching' in df.columns:\n",
    "    def classify_bleaching(p):\n",
    "        if p >= 60:\n",
    "            return \"Severe\"\n",
    "        elif p >= 15:\n",
    "            return \"Moderate\"\n",
    "        else:\n",
    "            return \"Mild\"\n",
    "    \n",
    "    df[\"Bleaching_Level\"] = df[\"Percent_Bleaching\"].apply(classify_bleaching)\n",
    "    \n",
    "    print(\"\\nðŸŒŠ Bleaching Level Distribution:\\n\")\n",
    "    print(df[\"Bleaching_Level\"].value_counts())\n",
    "\n",
    "# Step 8: Optional â€” show correlations\n",
    "print(\"\\nðŸ“ˆ Correlation Matrix (numerical columns only):\\n\")\n",
    "print(df.corr(numeric_only=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5faf7368-a464-454e-abe8-8d5768846d95",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# âœ… STEP 1: Import libraries\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# âœ… STEP 2: Load dataset\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# âœ… STEP 1: Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# âœ… STEP 2: Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# âœ… STEP 3: Convert numeric-like columns to floats\n",
    "numeric_cols = ['Temperature_Mean', 'Windspeed', 'SSTA', 'SSTA_DHW', 'TSA', 'TSA_DHWMean']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')  # convert invalid entries to NaN\n",
    "\n",
    "# âœ… STEP 4: Drop rows with missing values in those important columns\n",
    "df.dropna(subset=numeric_cols, inplace=True)\n",
    "\n",
    "# âœ… STEP 5: Encode categorical columns\n",
    "# Identify which columns are categorical\n",
    "cat_cols = ['Ocean', 'Location_Continent', 'Exposure']\n",
    "\n",
    "# Use one-hot encoding for categorical variables\n",
    "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# âœ… STEP 6: Create target variable\n",
    "# If you already have 'Bleaching_Level' column, skip this step\n",
    "if 'Percent_Bleaching' in df.columns and 'Bleaching_Level' not in df.columns:\n",
    "    def classify_bleaching(p):\n",
    "        if p >= 60:\n",
    "            return \"Severe\"\n",
    "        elif p >= 15:\n",
    "            return \"Moderate\"\n",
    "        else:\n",
    "            return \"Mild\"\n",
    "    df['Bleaching_Level'] = df['Percent_Bleaching'].apply(classify_bleaching)\n",
    "\n",
    "# âœ… STEP 7: Encode target variable numerically\n",
    "label_encoder = LabelEncoder()\n",
    "df['Bleaching_Level_Encoded'] = label_encoder.fit_transform(df['Bleaching_Level'])\n",
    "\n",
    "# âœ… STEP 8: Define features (X) and target (y)\n",
    "X = df.drop(columns=['Percent_Bleaching', 'Bleaching_Level', 'Bleaching_Level_Encoded'])\n",
    "y = df['Bleaching_Level_Encoded']\n",
    "\n",
    "# âœ… STEP 9: Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# âœ… STEP 10: Show preprocessing summary\n",
    "print(\"âœ… Preprocessing complete!\\n\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(\"\\nTarget label mapping:\")\n",
    "for i, cls in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i} -> {cls}\")\n",
    "\n",
    "print(\"\\nSample features:\\n\", X_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61f98eb3-ce3b-43f0-b567-e6e58a4867ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.2-cp312-cp312-win_amd64.whl (8.7 MB)\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/8.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.7 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/8.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.7 MB 399.6 kB/s eta 0:00:21\n",
      "   -- ------------------------------------- 0.5/8.7 MB 399.6 kB/s eta 0:00:21\n",
      "   -- ------------------------------------- 0.5/8.7 MB 399.6 kB/s eta 0:00:21\n",
      "   --- ------------------------------------ 0.8/8.7 MB 441.7 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 0.8/8.7 MB 441.7 kB/s eta 0:00:18\n",
      "   --- ------------------------------------ 0.8/8.7 MB 441.7 kB/s eta 0:00:18\n",
      "   ---- ----------------------------------- 1.0/8.7 MB 409.4 kB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 1.0/8.7 MB 409.4 kB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 1.0/8.7 MB 409.4 kB/s eta 0:00:19\n",
      "   ---- ----------------------------------- 1.0/8.7 MB 409.4 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 1.3/8.7 MB 399.5 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 1.3/8.7 MB 399.5 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 1.3/8.7 MB 399.5 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 1.3/8.7 MB 399.5 kB/s eta 0:00:19\n",
      "   ------ --------------------------------- 1.3/8.7 MB 399.5 kB/s eta 0:00:19\n",
      "   ------- -------------------------------- 1.6/8.7 MB 355.5 kB/s eta 0:00:21\n",
      "   ------- -------------------------------- 1.6/8.7 MB 355.5 kB/s eta 0:00:21\n",
      "   ------- -------------------------------- 1.6/8.7 MB 355.5 kB/s eta 0:00:21\n",
      "   ------- -------------------------------- 1.6/8.7 MB 355.5 kB/s eta 0:00:21\n",
      "   ------- -------------------------------- 1.6/8.7 MB 355.5 kB/s eta 0:00:21\n",
      "   -------- ------------------------------- 1.8/8.7 MB 334.4 kB/s eta 0:00:21\n",
      "   -------- ------------------------------- 1.8/8.7 MB 334.4 kB/s eta 0:00:21\n",
      "   -------- ------------------------------- 1.8/8.7 MB 334.4 kB/s eta 0:00:21\n",
      "   --------- ------------------------------ 2.1/8.7 MB 347.5 kB/s eta 0:00:20\n",
      "   --------- ------------------------------ 2.1/8.7 MB 347.5 kB/s eta 0:00:20\n",
      "   --------- ------------------------------ 2.1/8.7 MB 347.5 kB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 2.4/8.7 MB 359.8 kB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 2.4/8.7 MB 359.8 kB/s eta 0:00:18\n",
      "   ---------- ----------------------------- 2.4/8.7 MB 359.8 kB/s eta 0:00:18\n",
      "   ------------ --------------------------- 2.6/8.7 MB 362.1 kB/s eta 0:00:17\n",
      "   ------------ --------------------------- 2.6/8.7 MB 362.1 kB/s eta 0:00:17\n",
      "   ------------ --------------------------- 2.6/8.7 MB 362.1 kB/s eta 0:00:17\n",
      "   -------------- ------------------------- 3.1/8.7 MB 400.3 kB/s eta 0:00:14\n",
      "   --------------- ------------------------ 3.4/8.7 MB 423.9 kB/s eta 0:00:13\n",
      "   ---------------- ----------------------- 3.7/8.7 MB 450.7 kB/s eta 0:00:12\n",
      "   ---------------- ----------------------- 3.7/8.7 MB 450.7 kB/s eta 0:00:12\n",
      "   ------------------ --------------------- 3.9/8.7 MB 460.6 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 3.9/8.7 MB 460.6 kB/s eta 0:00:11\n",
      "   ------------------ --------------------- 3.9/8.7 MB 460.6 kB/s eta 0:00:11\n",
      "   ------------------- -------------------- 4.2/8.7 MB 465.2 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 4.2/8.7 MB 465.2 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 4.2/8.7 MB 465.2 kB/s eta 0:00:10\n",
      "   ------------------- -------------------- 4.2/8.7 MB 465.2 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 4.5/8.7 MB 451.9 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 4.5/8.7 MB 451.9 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 4.5/8.7 MB 451.9 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 4.5/8.7 MB 451.9 kB/s eta 0:00:10\n",
      "   -------------------- ------------------- 4.5/8.7 MB 451.9 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.7/8.7 MB 432.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.7/8.7 MB 432.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.7/8.7 MB 432.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.7/8.7 MB 432.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.7/8.7 MB 432.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.7/8.7 MB 432.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.7/8.7 MB 432.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.7/8.7 MB 432.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.7/8.7 MB 432.8 kB/s eta 0:00:10\n",
      "   --------------------- ------------------ 4.7/8.7 MB 432.8 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 5.0/8.7 MB 382.3 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 5.0/8.7 MB 382.3 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 5.0/8.7 MB 382.3 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 5.0/8.7 MB 382.3 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 5.0/8.7 MB 382.3 kB/s eta 0:00:10\n",
      "   ---------------------- ----------------- 5.0/8.7 MB 382.3 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 5.2/8.7 MB 363.5 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 5.2/8.7 MB 363.5 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 5.2/8.7 MB 363.5 kB/s eta 0:00:10\n",
      "   ------------------------ --------------- 5.2/8.7 MB 363.5 kB/s eta 0:00:10\n",
      "   ------------------------- -------------- 5.5/8.7 MB 362.4 kB/s eta 0:00:09\n",
      "   ------------------------- -------------- 5.5/8.7 MB 362.4 kB/s eta 0:00:09\n",
      "   -------------------------- ------------- 5.8/8.7 MB 368.2 kB/s eta 0:00:09\n",
      "   -------------------------- ------------- 5.8/8.7 MB 368.2 kB/s eta 0:00:09\n",
      "   --------------------------- ------------ 6.0/8.7 MB 375.9 kB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 6.3/8.7 MB 387.8 kB/s eta 0:00:07\n",
      "   ------------------------------ --------- 6.6/8.7 MB 399.5 kB/s eta 0:00:06\n",
      "   ------------------------------- -------- 6.8/8.7 MB 412.8 kB/s eta 0:00:05\n",
      "   ------------------------------- -------- 6.8/8.7 MB 412.8 kB/s eta 0:00:05\n",
      "   -------------------------------- ------- 7.1/8.7 MB 417.8 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 7.3/8.7 MB 428.2 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 7.3/8.7 MB 428.2 kB/s eta 0:00:04\n",
      "   --------------------------------- ------ 7.3/8.7 MB 428.2 kB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 7.6/8.7 MB 429.4 kB/s eta 0:00:03\n",
      "   ------------------------------------ --- 7.9/8.7 MB 439.1 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 8.1/8.7 MB 447.8 kB/s eta 0:00:02\n",
      "   ------------------------------------- -- 8.1/8.7 MB 447.8 kB/s eta 0:00:02\n",
      "   ---------------------------------------  8.7/8.7 MB 466.4 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.7/8.7 MB 468.5 kB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scikit-learn\n",
      "\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   -------------------- ------------------- 1/2 [scikit-learn]\n",
      "   ---------------------------------------- 2/2 [scikit-learn]\n",
      "\n",
      "Successfully installed scikit-learn-1.7.2 threadpoolctl-3.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0236cd14-1919-4034-a5b3-0f1b3b6d2965",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['Ocean', 'Location_Continent'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m cat_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOcean\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLocation_Continent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExposure\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Use one-hot encoding for categorical variables\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dummies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcat_cols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdrop_first\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# âœ… STEP 6: Create target variable\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# If you already have 'Bleaching_Level' column, skip this step\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPercent_Bleaching\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBleaching_Level\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\reshape\\encoding.py:169\u001b[0m, in \u001b[0;36mget_dummies\u001b[1;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput must be a list-like for parameter `columns`\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m     data_to_encode \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# validate prefixes and separator to avoid silently dropping cols\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_len\u001b[39m(item, name: \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[1;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['Ocean', 'Location_Continent'] not in index\""
     ]
    }
   ],
   "source": [
    "# âœ… STEP 1: Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# âœ… STEP 2: Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# âœ… STEP 3: Convert numeric-like columns to floats\n",
    "numeric_cols = ['Temperature_Mean', 'Windspeed', 'SSTA', 'SSTA_DHW', 'TSA', 'TSA_DHWMean']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')  # convert invalid entries to NaN\n",
    "\n",
    "# âœ… STEP 4: Drop rows with missing values in those important columns\n",
    "df.dropna(subset=numeric_cols, inplace=True)\n",
    "\n",
    "# âœ… STEP 5: Encode categorical columns\n",
    "# Identify which columns are categorical\n",
    "cat_cols = ['Ocean', 'Location_Continent', 'Exposure']\n",
    "\n",
    "# Use one-hot encoding for categorical variables\n",
    "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# âœ… STEP 6: Create target variable\n",
    "# If you already have 'Bleaching_Level' column, skip this step\n",
    "if 'Percent_Bleaching' in df.columns and 'Bleaching_Level' not in df.columns:\n",
    "    def classify_bleaching(p):\n",
    "        if p >= 60:\n",
    "            return \"Severe\"\n",
    "        elif p >= 15:\n",
    "            return \"Moderate\"\n",
    "        else:\n",
    "            return \"Mild\"\n",
    "    df['Bleaching_Level'] = df['Percent_Bleaching'].apply(classify_bleaching)\n",
    "\n",
    "# âœ… STEP 7: Encode target variable numerically\n",
    "label_encoder = LabelEncoder()\n",
    "df['Bleaching_Level_Encoded'] = label_encoder.fit_transform(df['Bleaching_Level'])\n",
    "\n",
    "# âœ… STEP 8: Define features (X) and target (y)\n",
    "X = df.drop(columns=['Percent_Bleaching', 'Bleaching_Level', 'Bleaching_Level_Encoded'])\n",
    "y = df['Bleaching_Level_Encoded']\n",
    "\n",
    "# âœ… STEP 9: Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# âœ… STEP 10: Show preprocessing summary\n",
    "print(\"âœ… Preprocessing complete!\\n\")\n",
    "print(f\"Shape of X_train: {X_train.shape}\")\n",
    "print(f\"Shape of X_test: {X_test.shape}\")\n",
    "print(\"\\nTarget label mapping:\")\n",
    "for i, cls in enumerate(label_encoder.classes_):\n",
    "    print(f\"{i} -> {cls}\")\n",
    "\n",
    "print(\"\\nSample features:\\n\", X_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6410dde2-0351-4abd-a6ed-fab7fd0d20a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Preprocessing complete!\n",
      "X_train shape: (999, 24), X_test shape: (250, 24)\n",
      "Target label mapping: {'Mild': np.int64(0), 'Moderate': np.int64(1), 'Severe': np.int64(2)}\n"
     ]
    }
   ],
   "source": [
    "# âœ… STEP 1: Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# âœ… STEP 2: Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# âœ… STEP 3: Convert numeric-like columns to float\n",
    "numeric_cols = [\n",
    "    'Latitude_Degrees', 'Longitude_Degrees', 'Turbidity', 'Cyclone_Frequency',\n",
    "    'Temperature_Mean', 'Windspeed', 'SSTA', 'SSTA_DHW', 'TSA', 'TSA_DHWMean'\n",
    "]\n",
    "\n",
    "for col in numeric_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# Drop rows with missing numeric values\n",
    "df.dropna(subset=numeric_cols + ['Percent_Bleaching'], inplace=True)\n",
    "\n",
    "# âœ… STEP 4: Encode categorical columns\n",
    "cat_cols = ['Ocean_Name', 'Exposure']\n",
    "df = pd.get_dummies(df, columns=cat_cols, drop_first=True)\n",
    "\n",
    "# âœ… STEP 5: Create severity target\n",
    "def classify_bleaching(p):\n",
    "    if p >= 60:\n",
    "        return \"Severe\"\n",
    "    elif p >= 15:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"Mild\"\n",
    "\n",
    "df['Bleaching_Level'] = df['Percent_Bleaching'].apply(classify_bleaching)\n",
    "\n",
    "# Encode target numerically\n",
    "label_encoder = LabelEncoder()\n",
    "df['Bleaching_Level_Encoded'] = label_encoder.fit_transform(df['Bleaching_Level'])\n",
    "\n",
    "# âœ… STEP 6: Features and target\n",
    "X = df.drop(columns=['Percent_Bleaching', 'Bleaching_Level', 'Bleaching_Level_Encoded'])\n",
    "y = df['Bleaching_Level_Encoded']\n",
    "\n",
    "# âœ… STEP 7: Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(\"âœ… Preprocessing complete!\")\n",
    "print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "print(\"Target label mapping:\", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60bdfa10-0d16-4bde-bd38-df7b723205a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m classification_report, confusion_matrix\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# âœ… STEP 1: Train Random Forest\u001b[39;00m\n\u001b[0;32m      7\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# âœ… STEP 1: Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# âœ… STEP 2: Predict on test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# âœ… STEP 3: Classification report\n",
    "print(\"\\nðŸŒŠ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# âœ… STEP 4: Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_, cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# âœ… STEP 5: Feature importance\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feat_df = feat_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_df)\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b09f60d-9a37-4360-9ab9-f67ea0b8c39a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (2.2.2)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\adith\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
      "Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db44ee4d-9836-4545-9460-cb0fe2421d52",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Belize and west Caribbean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# âœ… STEP 1: Train Random Forest\u001b[39;00m\n\u001b[0;32m      7\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# âœ… STEP 2: Predict on test set\u001b[39;00m\n\u001b[0;32m     11\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1365\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1358\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1361\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1362\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1363\u001b[0m     )\n\u001b[0;32m   1364\u001b[0m ):\n\u001b[1;32m-> 1365\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:359\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 359\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    362\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    363\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    372\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:2971\u001b[0m, in \u001b[0;36mvalidate_data\u001b[1;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[0;32m   2969\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m   2970\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2971\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2972\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m   2974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1368\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1362\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1363\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1364\u001b[0m     )\n\u001b[0;32m   1366\u001b[0m ensure_all_finite \u001b[38;5;241m=\u001b[39m _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[1;32m-> 1368\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1383\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1385\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1387\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:971\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[0;32m    970\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[1;32m--> 971\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    972\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[0;32m    973\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   6639\u001b[0m     ]\n\u001b[0;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[1;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[0;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[1;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[0;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[1;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[0;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[0;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[1;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[0;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[1;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[0;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[1;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[0;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[1;34m(values, dtype, copy)\u001b[0m\n\u001b[0;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[0;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\dtypes\\astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[1;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'Belize and west Caribbean'"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# âœ… STEP 1: Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# âœ… STEP 2: Predict on test set\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# âœ… STEP 3: Classification report\n",
    "print(\"\\nðŸŒŠ Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# âœ… STEP 4: Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(6,5))\n",
    "sns.heatmap(cm, annot=True, fmt='d', xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_, cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# âœ… STEP 5: Feature importance\n",
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feat_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feat_df = feat_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feat_df)\n",
    "plt.title(\"Feature Importances\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2f6a7be0-b142-43d8-b202-0662c47cd2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions: ['Moderate' 'Severe' 'Severe' 'Mild' 'Mild' 'Mild' 'Mild' 'Moderate'\n",
      " 'Moderate' 'Severe']\n",
      "Feature columns used: ['Temperature_Mean', 'Windspeed', 'TSA', 'Ocean_Name_Atlantic', 'Ocean_Name_Indian', 'Ocean_Name_Pacific', 'Ocean_Name_Red Sea', 'Exposure_Sheltered', 'Exposure_Sometimes']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# âœ… Load dataset\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "\n",
    "# âœ… Convert numeric columns\n",
    "num_cols = ['Temperature_Mean', 'Windspeed', 'TSA']\n",
    "for col in num_cols:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# âœ… Drop rows with missing numeric data\n",
    "df.dropna(subset=num_cols, inplace=True)\n",
    "\n",
    "# âœ… Encode target variable\n",
    "def classify_bleaching(p):\n",
    "    if p >= 60:\n",
    "        return \"Severe\"\n",
    "    elif p >= 15:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"Mild\"\n",
    "\n",
    "df['Bleaching_Level'] = df['Percent_Bleaching'].apply(classify_bleaching)\n",
    "\n",
    "# Encode labels as numbers\n",
    "label_encoder = LabelEncoder()\n",
    "df['Bleaching_Level_enc'] = label_encoder.fit_transform(df['Bleaching_Level'])\n",
    "\n",
    "# âœ… Select only relevant features\n",
    "features = ['Ocean_Name', 'Exposure', 'Temperature_Mean', 'Windspeed', 'TSA']\n",
    "X = df[features]\n",
    "y = df['Bleaching_Level_enc']\n",
    "\n",
    "# âœ… One-hot encode categorical features\n",
    "X = pd.get_dummies(X, columns=['Ocean_Name', 'Exposure'], drop_first=True)\n",
    "\n",
    "# âœ… Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# âœ… Train Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# âœ… Test\n",
    "y_pred = rf.predict(X_test)\n",
    "\n",
    "# Decode predictions\n",
    "pred_class = label_encoder.inverse_transform(y_pred)\n",
    "\n",
    "# Quick summary\n",
    "print(\"Sample predictions:\", pred_class[:10])\n",
    "print(\"Feature columns used:\", X_train.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ddd1ed79-6313-41c2-b8da-ce2ca8186a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Bleaching Levels: ['Severe']\n"
     ]
    }
   ],
   "source": [
    "# Example new coral site(s)\n",
    "new_sites = pd.DataFrame([{\n",
    "    'Temperature_Mean': 289.45,\n",
    "    'Windspeed': 5,\n",
    "    'TSA': 1.5,\n",
    "    'Ocean_Name': 'Pacific',\n",
    "    'Exposure': 'Sheltered'\n",
    "}])\n",
    "\n",
    "# One-hot encode categorical features (match training columns)\n",
    "new_sites_encoded = pd.get_dummies(new_sites, columns=['Ocean_Name', 'Exposure'], drop_first=True)\n",
    "\n",
    "# Add any missing columns (from training) with zeros\n",
    "for col in X_train.columns:\n",
    "    if col not in new_sites_encoded.columns:\n",
    "        new_sites_encoded[col] = 0\n",
    "\n",
    "# Ensure the column order matches training set\n",
    "new_sites_encoded = new_sites_encoded[X_train.columns]\n",
    "\n",
    "# Predict\n",
    "pred_labels = rf.predict(new_sites_encoded)\n",
    "pred_classes = label_encoder.inverse_transform(pred_labels)\n",
    "\n",
    "print(\"Predicted Bleaching Levels:\", pred_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4b3b44-a0ae-4120-a9f6-085eb0172536",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad7fabd-e0f1-47b9-935d-a9c3ba413911",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
